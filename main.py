import os
import re
import uuid
import tempfile
import time
import json
import openpyxl
import requests
from datetime import datetime
from urllib.parse import urljoin, urlparse, urlencode, urlsplit, urlunsplit, parse_qs
from lxml import html
from openpyxl.styles import PatternFill
from dotenv import load_dotenv
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from db_driver import ConfigDBDriver
from send_email import send_email_notification
# Load env
load_dotenv()
BOT_TOKEN = os.getenv("BOT_TOKEN")
BASE_URL = os.getenv("BASE_URL", "http://localhost")

OUTPUT_FOLDER = "output_files"
os.makedirs(OUTPUT_FOLDER, exist_ok=True)


# ============================================================
# üß† DB Config Loader
# ============================================================
def get_website_config(url):
    db = ConfigDBDriver()
    rows = db.cursor.execute("SELECT website, config_json FROM configs").fetchall()
    for row in rows:
        website = row["website"]
        if website in url:
            db.close()
            return website, json.loads(row["config_json"])
    db.close()
    return None, None


# ============================================================
# üåê Selenium Loader
# ============================================================
def get_rendered_html(url, config):
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-gpu")
    tempdir = tempfile.mkdtemp()
    chrome_options.add_argument(f"--user-data-dir={tempdir}")
    driver = None
    try:
        driver = webdriver.Chrome(options=chrome_options)
        driver.get(url)

        wait_time = config.get("wait_time", 6)
        lazy_scroll = config.get("lazy_scroll", False)
        max_scrolls = config.get("max_scrolls", 10)
        scroll_pause = config.get("scroll_pause", 2)
        page_ready_xpath = config.get("page_ready_xpath", "//*")

        try:
            WebDriverWait(driver, wait_time).until(
                EC.presence_of_all_elements_located((By.XPATH, page_ready_xpath))
            )
        except TimeoutException:
            print("‚ö†Ô∏è Timeout waiting for page load")

        if lazy_scroll:
            last_height = driver.execute_script("return document.body.scrollHeight")
            for _ in range(max_scrolls):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(scroll_pause)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height

        html_content = driver.page_source
        driver.quit()
        return html_content
    except WebDriverException as e:
        print(f"‚ö†Ô∏è Selenium Error: {e}")
        if driver:
            driver.quit()
        return None
    finally:
        if driver:
            driver.quit()
        os.rmdir(tempdir)

# ============================================================
# üß© Property Parser
# ============================================================
# ============================================================
# üß© Property Parser (fixed title spacing)
# ============================================================
def parse_property_with_config(url, config, download_folder="images"):
    result = {"–°—Å—ã–ª–∫–∞ –Ω–∞ –æ–±—ä–µ–∫—Ç": url}
    os.makedirs(download_folder, exist_ok=True)

    try:
        html_content = get_rendered_html(url, config)
        if not html_content:
            r = requests.get(url)
            html_content = r.text if r.status_code == 200 else ""

        tree = html.fromstring(html_content)
        fields = config.get("fields", {})

        for field_name, field_data in fields.items():
            xpath = field_data.get("xpath")
            transform = field_data.get("transform")

            # --- Handle photo fields separately ---
            if field_name in ["–§–æ—Ç–æ_—Å—Å—ã–ª–∫–∏", "–§–æ—Ç–æ_—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ_–Ω–∞–∑–≤–∞–Ω–∏—è"] and xpath:
                urls = tree.xpath(xpath)
                if not urls:
                    result[field_name] = "ERROR"
                    continue

                downloaded_files = []
                for u in urls:
                    u_clean = u.split(",")[0].strip()
                    filename = f"{uuid.uuid4().hex}_{os.path.basename(u_clean)}"
                    local_path = os.path.join(download_folder, filename)

                    if not os.path.exists(local_path):
                        try:
                            resp = requests.get(u_clean, timeout=10)
                            if resp.status_code == 200:
                                with open(local_path, "wb") as f:
                                    f.write(resp.content)
                        except Exception as e:
                            print(f"‚ö†Ô∏è Failed to download image {u_clean}: {e}")
                            continue

                    downloaded_files.append((local_path, filename))

                if downloaded_files:
                    result["–§–æ—Ç–æ_—Å—Å—ã–ª–∫–∏"] = ";".join(
                        [f"{BASE_URL}/images/{n}" for p, n in downloaded_files]
                    )
                    result["–§–æ—Ç–æ_—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ_–Ω–∞–∑–≤–∞–Ω–∏—è"] = ";".join(
                        [n for p, n in downloaded_files]
                    )
                else:
                    result["–§–æ—Ç–æ_—Å—Å—ã–ª–∫–∏"] = "ERROR"
                    result["–§–æ—Ç–æ_—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ_–Ω–∞–∑–≤–∞–Ω–∏—è"] = "ERROR"
                continue

            # --- Skip missing xpath ---
            if not xpath:
                result[field_name] = "ERROR"
                continue

            # --- Extract values ---
            values = tree.xpath(xpath)
            if not values:
                result[field_name] = "ERROR"
                continue

            cleaned_values = []
            for v in values:
                text_value = v.text_content().strip() if hasattr(v, "text_content") else str(v).strip()
                cleaned_values.append(text_value)

            combined_value = "\n".join(cleaned_values)

            # ‚úÖ Normalize multiple spaces in title
            if field_name == "–ù–∞–∑–≤–∞–Ω–∏–µ":
                combined_value = re.sub(r"\s+", " ", combined_value.strip())

            # --- Apply transform if defined ---
            if transform:
                try:
                    safe_globals = {"re": re, "__builtins__": {}}
                    combined_value = eval(transform, safe_globals, {"value": combined_value})
                except Exception as e:
                    combined_value = f"TransformError: {e}"

            result[field_name] = combined_value

        return result, None

    except Exception as e:
        return None, f"ERROR: {str(e)}"


# ============================================================
# üíæ Excel Export (fixed column order in Russian)
# ============================================================
def save_to_excel(properties, filename):
    # ‚úÖ Desired column order (Russian)
    desired_order = [
        "–°—Å—ã–ª–∫–∞ –Ω–∞ –æ–±—ä–µ–∫—Ç", "–ù–∞–∑–≤–∞–Ω–∏–µ", "–¶–µ–Ω–∞", "–í–∞–ª—é—Ç–∞", "–ü–ª–æ—â–∞–¥—å", "–ü–ª–æ—â–∞–¥—å –∑–µ–º–ª–∏",
        "–¢–∏–ø –æ–±—ä–µ–∫—Ç–∞", "–ì–æ–¥ –ø–æ—Å—Ç—Ä–æ–π–∫–∏", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç", "–û–ø–∏—Å–∞–Ω–∏–µ", "–ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞",
        "–°/—É", "–≠—Ç–∞–∂", "–õ–æ–∫–∞—Ü–∏—è", "–ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã", "–§–æ—Ç–æ_—Å—Å—ã–ª–∫–∏", "–§–æ—Ç–æ_—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ_–Ω–∞–∑–≤–∞–Ω–∏—è",
        "–ö–æ–Ω—Ç–∞–∫—Ç–Ω–æ–µ –ª–∏—Ü–æ", "–¢–µ–ª–µ—Ñ–æ–Ω –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–≥–æ –ª–∏—Ü–∞", "–ö–æ–º–ø–∞–Ω–∏—è", "–¢–µ–ª–µ—Ñ–æ–Ω –∫–æ–º–ø–∞–Ω–∏–∏"
    ]

    # Make sure every property has all columns
    for prop in properties:
        for col in desired_order:
            if col not in prop:
                prop[col] = "ERROR"

    wb = openpyxl.Workbook()
    ws = wb.active
    ws.append(desired_order)
    red_fill = PatternFill(start_color="FFFF0000", end_color="FFFF0000", fill_type="solid")

    for prop in properties:
        row = [prop.get(col, "ERROR") for col in desired_order]
        ws.append(row)
        for i, value in enumerate(row, start=1):
            if value == "ERROR":
                ws.cell(row=ws.max_row, column=i).fill = red_fill

    file_path = os.path.join(OUTPUT_FOLDER, filename)
    wb.save(file_path)
    return file_path

# ============================================================
# üß© List Page Parser with Auto Pagination (Next Button Supported)
# ============================================================
def parse_list_page(base_url, config):
    print(f"üåç Fetching list pages from: {base_url}")

    properties = []
    seen_first = None
    page = 1
    page_query = config.get("page_query")
    next_button_xpath = config.get("next_page_xpath")  # ‚úÖ NEW

    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--window-size=1920,1080")

    driver = webdriver.Chrome(options=chrome_options)
    driver.get(base_url)
    time.sleep(3)

    while True:
        print(f"\nüîÑ Loading page {page}...")

        # Extract property links
        html_content = driver.page_source
        tree = html.fromstring(html_content)
        property_links = tree.xpath(config.get("list_page_check", ""))

        if not property_links:
            print("üö´ No property links found ‚Üí stopping pagination.")
            break

        # Avoid infinite loop
        first_url = urljoin(base_url, property_links[0])
        if seen_first == first_url:
            print("üõë Same first record as previous ‚Üí last page reached.")
            break
        seen_first = first_url

        for idx, link in enumerate(property_links, start=1):
            full_url = urljoin(base_url, link)
            print(f"‚û°Ô∏è [{idx}/{len(property_links)}] {full_url}")
            data, error = parse_property_with_config(full_url, config)
            if data:
                properties.append(data)
            else:
                print(f"‚ùå Error parsing property: {error}")

        # Check for next page
        # Check for next page
        if next_button_xpath:
            try:
                next_btn = WebDriverWait(driver, 5).until(
                    EC.presence_of_element_located((By.XPATH, next_button_xpath))
                )
                driver.execute_script("arguments[0].scrollIntoView(true);", next_btn)
                time.sleep(1)
                # üí• Use JS click to avoid interception errors
                driver.execute_script("arguments[0].click();", next_btn)
                print("üëâ Clicked next page button via JS.")
                time.sleep(config.get("scroll_pause", 2))
                page += 1
                continue
            except Exception as e:
                print(f"üõë No next page button found or not clickable: {e}")
                break

        elif page_query:
            # Fallback to query pagination if defined
            parts = list(urlsplit(base_url))
            query = parse_qs(parts[3])
            query[page_query] = [str(page + 1)]
            parts[3] = urlencode(query, doseq=True)
            next_page_url = urlunsplit(parts)
            print(f"‚û°Ô∏è Loading next page via query: {next_page_url}")
            driver.get(next_page_url)
            time.sleep(2)
            page += 1
            continue
        else:
            break

    driver.quit()
    return properties


# ============================================================
# ü§ñ Telegram Bot Handlers
# ============================================================
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("üëã –û—Ç–ø—Ä–∞–≤—å—Ç–µ URL –æ–±—ä–µ–∫—Ç–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç–∏/–ª–∏—Å—Ç–∏–Ω–≥–∞. –Ø —Å–∫–∞—á–∞—é –∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É—é –≤—Å—ë –≤ Excel.")


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    url = update.message.text.strip()
    await update.message.reply_text("‚úÖ –ü—Ä–∏–Ω—è—Ç–æ, —Å–æ–±–∏—Ä–∞—é‚Ä¶")
    print(f"üëÄ Input URL: {url}")

    domain, config = get_website_config(url)
    if not config:
        await update.message.reply_text("‚ùå –ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω. –î–æ–±–∞–≤—å—Ç–µ –≤ –ø–∞–Ω–µ–ª–∏.")
        return

    # Try single property first
    data, error = parse_property_with_config(url, config)
    if data and data.get("–ù–∞–∑–≤–∞–Ω–∏–µ") != "ERROR":
        properties = [data]
    else:
        properties = parse_list_page(url, config)

    if not properties:
        await update.message.reply_text("‚ùå –ù–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞.")
        return

    filename = f"{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}_properties.xlsx"
    file_path = save_to_excel(properties, filename)
    import pandas as pd
    df = pd.read_excel(file_path, engine="openpyxl")
    error_count = df.apply(lambda x: x.astype(str).str.contains("error", case=False, na=False)).sum().sum()
    await update.message.reply_text(
        f"‚úÖ –ì–æ—Ç–æ–≤–æ: {len(properties)} –æ–±—ä–µ–∫—Ç–∞ {error_count} —Å –æ—à–∏–±–∫–∞–º–∏. –°–∫–∞—á–∞—Ç—å Excel: \nüìÇ {BASE_URL}/output_file/{filename}"
    )
    send_email_notification("üöÄ Bot Notification", f"‚úÖ –ì–æ—Ç–æ–≤–æ: {len(properties)} –æ–±—ä–µ–∫—Ç–∞ {error_count} —Å –æ—à–∏–±–∫–∞–º–∏. –°–∫–∞—á–∞—Ç—å Excel: \nüìÇ {BASE_URL}/output_file/{filename}")


# ============================================================
# üöÄ Launch Bot
# ============================================================
if __name__ == "__main__":
    print("ü§ñ Bot running ‚Äî config-driven, paginated scraper active...")
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    app.add_handler(CommandHandler("start", start))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    app.run_polling()
